{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":19991,"databundleVersionId":1117522,"sourceType":"competition"},{"sourceId":127219,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":107116,"modelId":131461}],"dockerImageVersionId":30763,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm\nimport warnings\n\n# Ignore all warnings\nwarnings.filterwarnings('ignore')\n\n# Constants\nIMG_HEIGHT = 256\nIMG_WIDTH = 256\nBATCH_SIZE = 32\nEPOCHS = 30\n\n# Set your dataset paths\nCOVER_DIR = '/kaggle/input/alaska2-image-steganalysis/Cover'\nJMIPOD_DIR = '/kaggle/input/alaska2-image-steganalysis/JMiPOD'\nJUNIWARD_DIR = '/kaggle/input/alaska2-image-steganalysis/JUNIWARD'\nUERD_DIR = '/kaggle/input/alaska2-image-steganalysis/UERD'\n\n# Load dataset paths and create labels\ndef load_data_paths():\n    image_paths = []\n    labels = []\n    groups = []\n\n    # Load cover images\n    for img_name in os.listdir(COVER_DIR):\n        img_path = os.path.join(COVER_DIR, img_name)\n        image_paths.append(img_path)\n        labels.append(0)  # Cover images\n        groups.append(img_name)  # Use image name as group identifier\n\n    # Load stego images\n    for img_name in os.listdir(JMIPOD_DIR):\n        img_path = os.path.join(JMIPOD_DIR, img_name)\n        image_paths.append(img_path)\n        labels.append(1)  # JMiPOD images\n        groups.append(img_name)  # Group by cover image name\n\n    for img_name in os.listdir(JUNIWARD_DIR):\n        img_path = os.path.join(JUNIWARD_DIR, img_name)\n        image_paths.append(img_path)\n        labels.append(1)  # JUNIWARD images\n        groups.append(img_name)  # Group by cover image name\n\n    for img_name in os.listdir(UERD_DIR):\n        img_path = os.path.join(UERD_DIR, img_name)\n        image_paths.append(img_path)\n        labels.append(1)  # UERD images\n        groups.append(img_name)  # Group by cover image name\n\n    return np.array(image_paths), np.array(labels), np.array(groups)\n\n# Data Augmentation\ndef augment(image):\n    # Resize image\n    image = cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH))\n\n    # Random flip\n    if np.random.rand() > 0.5:\n        image = cv2.flip(image, 1)  # horizontal flip\n\n    # Random rotation\n    angle = np.random.randint(-10, 11)  # Rotate between -10 and +10 degrees\n    center = (IMG_HEIGHT // 2, IMG_WIDTH // 2)\n    matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n    image = cv2.warpAffine(image, matrix, (IMG_HEIGHT, IMG_WIDTH))\n\n    return image\n\n# Custom Data Generator\nclass DataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, image_paths, labels, batch_size, shuffle=True):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.indices = np.arange(len(self.image_paths))\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(np.floor(len(self.image_paths) / self.batch_size))\n\n    def __getitem__(self, index):\n        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n        return self.__data_generation(batch_indices)\n\n    def on_epoch_end(self):\n        if self.shuffle:\n            np.random.shuffle(self.indices)\n\n    def __data_generation(self, batch_indices):\n        batch_images = np.empty((self.batch_size, IMG_HEIGHT, IMG_WIDTH, 3))\n        batch_labels = np.empty((self.batch_size,))\n\n        for i, idx in enumerate(batch_indices):\n            image = cv2.imread(self.image_paths[idx])\n            image = augment(image)  # Apply augmentation\n            batch_images[i,] = image / 255.0  # Normalize to [0, 1]\n            batch_labels[i] = self.labels[idx]\n\n        return batch_images, batch_labels\n\n# Load Data\nimage_paths, labels, groups = load_data_paths()\n\n# Group K-Fold Split\ngkf = GroupKFold(n_splits=5)\nfolds = list(gkf.split(image_paths, labels, groups=groups))\n\n# Define the model using a pretrained EfficientNet\ndef create_model():\n    base_model = tf.keras.applications.EfficientNetB0(\n        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n        include_top=False,\n        weights='imagenet'\n    )\n    base_model.trainable = False  # Freeze the base model\n\n    inputs = tf.keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n    x = base_model(inputs, training=False)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)  # Binary classification\n    model = tf.keras.Model(inputs, outputs)\n    \n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n# Training Loop\ndef train_model():\n    for fold, (train_idx, val_idx) in enumerate(folds):\n        print(f\"Training fold {fold + 1}/{len(folds)}\")\n\n        # Split data into training and validation\n        X_train_paths, X_val_paths = image_paths[train_idx], image_paths[val_idx]\n        y_train, y_val = labels[train_idx], labels[val_idx]\n\n        # Create generators\n        train_generator = DataGenerator(X_train_paths, y_train, batch_size=BATCH_SIZE)\n        val_generator = DataGenerator(X_val_paths, y_val, batch_size=BATCH_SIZE, shuffle=False)\n\n        model = create_model()\n\n        # Define the checkpoint callback to save the entire model after every epoch\n        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n            filepath=f'model_fold_{fold + 1}_epoch_{{epoch:02d}}.keras',  # Save as .keras\n            save_weights_only=False,  # Save the entire model\n            save_best_only=False,  # Save every epoch\n            mode='auto',\n            verbose=1\n        )\n\n        # Training\n        model.fit(train_generator, \n                  validation_data=val_generator,\n                  epochs=EPOCHS,\n                  verbose=1,\n                  callbacks=[checkpoint_callback])  # Add the checkpoint callback\n\n        # Evaluate on validation set\n        val_preds = model.predict(val_generator)\n        val_auc = roc_auc_score(y_val, val_preds)\n        print(f\"Fold {fold + 1} Validation AUC: {val_auc:.4f}\")\n\nif __name__ == \"__main__\":\n    train_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Training fold 1/5\nDownloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\nEpoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1728264662.359964     101 service.cc:145] XLA service 0x7887f8001700 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1728264662.360016     101 service.cc:153]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m   1/7500\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m68:25:21\u001b[0m 33s/step - accuracy: 0.7188 - loss: 0.6892","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1728264680.638097     101 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  17/7500\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:00:13\u001b[0m 483ms/step - accuracy: 0.7416 - loss: 0.6481","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport tensorflow as tf\n\n# Define the model loading function\ndef load_model(fold, epoch):\n    \"\"\"\n    Load the model from the specified fold and epoch.\n    Arguments:\n    - fold: The fold number (1-based index).\n    - epoch: The epoch number (1-based index).\n    \n    Returns:\n    - The loaded model.\n    \"\"\"\n    model_path = f'model_fold_{fold}_epoch_{epoch:02d}.keras'\n    \n    if os.path.exists(model_path):\n        print(f\"Loading model from {model_path}\")\n        model = tf.keras.models.load_model(model_path)\n        return model\n    else:\n        print(f\"No model found at {model_path}. Starting fresh training.\")\n        return create_model()\n\n# Modified training loop for resuming training\ndef resume_training(fold, start_epoch=0):\n    \"\"\"\n    Resume training for the given fold from the specified epoch.\n    Arguments:\n    - fold: The fold number to resume training on (1-based index).\n    - start_epoch: The epoch to start from (if model is loaded).\n    \"\"\"\n    print(f\"Resuming training for fold {fold} starting from epoch {start_epoch + 1}\")\n\n    # Split data into training and validation\n    train_idx, val_idx = folds[fold - 1]\n    X_train_paths, X_val_paths = image_paths[train_idx], image_paths[val_idx]\n    y_train, y_val = labels[train_idx], labels[val_idx]\n\n    # Create generators\n    train_generator = DataGenerator(X_train_paths, y_train, batch_size=BATCH_SIZE)\n    val_generator = DataGenerator(X_val_paths, y_val, batch_size=BATCH_SIZE, shuffle=False)\n\n    # Load the model from the latest epoch or create a new model\n    model = load_model(fold, start_epoch)\n\n    # Define the checkpoint callback to save the entire model after every epoch\n    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n        filepath=f'model_fold_{fold}_epoch_{{epoch:02d}}.keras',  # Save as .keras\n        save_weights_only=False,  # Save the entire model\n        save_best_only=False,  # Save every epoch\n        mode='auto',\n        verbose=1\n    )\n\n    # Training\n    model.fit(train_generator, \n              validation_data=val_generator,\n              initial_epoch=start_epoch,  # Start training from the specified epoch\n              epochs=EPOCHS,\n              verbose=1,\n              callbacks=[checkpoint_callback])  # Add the checkpoint callback\n\n    # Evaluate on validation set\n    val_preds = model.predict(val_generator)\n    val_auc = roc_auc_score(y_val, val_preds)\n    print(f\"Fold {fold} Validation AUC: {val_auc:.4f}\")\n\n# Example usage to resume training\n# Resume training from fold 1, epoch 10\nresume_training(fold=1, start_epoch=1)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import load_model\nfrom tqdm import tqdm\n\n# Define the test directory and path to your saved model\ntest_dir = '/kaggle/input/alaska2-image-steganalysis/Test'  # Update with the actual test folder path\nmodel_path = '/kaggle/input/mymodel7/keras/default/1/model_fold_1_epoch_07.keras'  # Path to your trained model\n\n# Load your pre-trained model\nmodel = load_model(model_path)\n\n# Image preprocessing parameters (match these with your training setup)\nIMG_SIZE = (256, 256)  # Image size used in training\nBATCH_SIZE = 32  # Adjust this based on available memory\n\n# Prepare the list for storing image IDs and their corresponding predictions\nsubmission_data = []\n\n# Get the list of test images\ntest_images = os.listdir(test_dir)\n\n# Preprocessing and prediction\nfor img_name in tqdm(test_images, desc=\"Processing images\"):\n    img_path = os.path.join(test_dir, img_name)\n    \n    # Load and preprocess the image\n    img = image.load_img(img_path, target_size=IMG_SIZE)\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n    img_array /= 255.0  # Normalize the same way as in training\n\n    # Make predictions\n    prediction = model.predict(img_array)\n    \n    # Assuming sigmoid activation, prediction gives the probability of stego, take the value\n    label = prediction[0][0]\n    \n    # Append the result to submission_data\n    submission_data.append([img_name, label])\n\n# Convert the submission data into a DataFrame\nsubmission_df = pd.DataFrame(submission_data, columns=['Id', 'Label'])\n\n# Save the DataFrame to a CSV file\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"Predictions saved to submission.csv\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}